{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Parking Tickets: An Exploratory Analysis\n",
    "\n",
    "New York City is a thriving metropolis. Just like most other metros its size, one of the biggest problems its citizens face is parking. The classic combination of a huge number of cars and cramped geography leads to a huge number of parking tickets. \n",
    "\n",
    "In an attempt to scientifically analyse this phenomenon, the NYC Police Department has collected data for parking tickets. Of these, the data files for multiple years are publicly available. it is required to perform some exploratory analysis on a part of this data. Spark will allow us to analyse the full files at high speeds as opposed to taking a series of random samples that will approximate the population. For the scope of this analysis, we will analyse the parking tickets over the year 2017.\n",
    "\n",
    "The purpose of this case study is to conduct an exploratory data analysis that will help you understand the data. The questions given below will guide your analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_232-cloudera/jre\"\n",
    "os.environ[\"SPARK_HOME\"]=\"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the data\n",
    "\n",
    "#For loading data from HDFS\n",
    "#nyc = spark.read.format('csv').options(header='true',inferschema='true').load(\"/nycdata.csv\")\n",
    "\n",
    "#For loading data from S3\n",
    "s3path = 's3a://upgrad-data/Parking_Violation_Tickets.csv'\n",
    "nyc = spark.read.format(\"csv\").option(\"header\", \"true\").load(s3path)\n",
    "\n",
    "nyc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspecting few lines of data\n",
    "\n",
    "nyc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the schema for the dataframe\n",
    "\n",
    "nyc.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing the schema\n",
    "\n",
    "nyc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking overall count\n",
    "\n",
    "nyc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking distinct count\n",
    "\n",
    "nyc.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that there are no duplicates in the file and the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting the data for 2017\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "nyc2017 = nyc.filter(year(nyc['Issue Date']) == 2017)\n",
    "nyc2017.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc2017.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above count shows the total number of records for year 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting data appropriately and creating new columns\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "## extacting hours, minutes and AM/PM values from Violation Time column\n",
    "nyc2017 = nyc2017.withColumn(\"Violation Hour\", col(\"Violation Time\").substr(1,2).cast(\"int\"))\n",
    "nyc2017 = nyc2017.withColumn(\"Violation Minutes\", col(\"Violation Time\").substr(3,2).cast(\"int\"))\n",
    "nyc2017 = nyc2017.withColumn(\"Violation ampm\", col(\"Violation Time\").substr(5,1))\n",
    "\n",
    "## converting AM / PM time to absolute hours ranging from 1-24\n",
    "## here we are adding 12 to pm values provided the time is not between 12 and 13 (12:00 pm to 1:00 pm)\n",
    "nyc2017 = nyc2017.withColumn(\"Violation Hour\",when(col(\"Violation Hour\")==12,when(col(\"Violation ampm\")=='A',0).otherwise(12)) \\\n",
    "                             .otherwise(when(col(\"Violation ampm\")=='P', col(\"Violation Hour\")+12) \\\n",
    "                                        .otherwise(col(\"Violation Hour\"))))\n",
    "\n",
    "## creating the unix timestamp using date and time wchich can be used later\n",
    "nyc2017 = nyc2017.withColumn(\"Long Time\", unix_timestamp(nyc2017[\"Issue Date\"])+ \\\n",
    "                             (nyc2017[\"Violation Hour\"]*3600) + (nyc2017[\"Violation Minutes\"]*60))\n",
    "\n",
    "## converting Issue Date columns from timestamp to date to use it in future as date\n",
    "nyc2017 = nyc2017.withColumn(\"Issue Date\", col(\"Issue Date\").cast(\"date\"))\n",
    "\n",
    "## after extracting the information the these columns are not required and hence dropped\n",
    "nyc2017 = nyc2017.drop(\"Violation Time\",\"Violation ampm\")\n",
    "\n",
    "## printing the schema now\n",
    "nyc2017.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking max min Violation hours we computed in dataframe\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "nyc2017.select(max(\"Violation Hour\"), min(\"Violation Hour\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking count of incorrect Violation hours which are greater than or equal to 24\n",
    "\n",
    "nyc2017.filter(col(\"Violation Hour\")>=24).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the count of records before dropping eror records\n",
    "\n",
    "nyc2017.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing incorrect values of Violation Hour (which are quiet less in number than overall count) from dataframe\n",
    "\n",
    "nyc2017 = nyc2017.filter(col(\"Violation Hour\")<24)\n",
    "nyc2017.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the record counts after deleting  error records for Violation Hour\n",
    "\n",
    "nyc2017.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking max min Violation hours we computed in dataframe\n",
    "\n",
    "nyc2017.select(max(\"Violation Minutes\"), min(\"Violation Minutes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for null values count\n",
    "\n",
    "from pyspark.sql.functions import isnan, count\n",
    "\n",
    "nyc2017.select([count(when(col(c).isNull(), c)).alias(c) for c in nyc2017.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping the null values (their count is very less)\n",
    "\n",
    "nyc2017 = nyc2017.na.drop()\n",
    "nyc2017.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the count for the 'NA' values \n",
    "\n",
    "nyc2017.select([count(when(col(c)=='NA', c)).alias(c) for c in nyc2017.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Find the total number of tickets for the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the count for tickets (we already know that records are distinct)\n",
    "## after initial data analysis and removing null data records\n",
    "\n",
    "nyc2017.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of tickets for 2017 are 5431834. This number is obtained after some (very less) number of records were removed due to error in data due to null values. Initially the number of records were 5431918."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Find out the number of unique states from where the cars that got parking tickets came. \n",
    "\n",
    "There is a numeric entry '99' in the column, which should be corrected. Replace it with the state having the maximum entries. Provide the number of unique states again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting distinct Registration State\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "nyc2017.select(countDistinct(col(\"Registration State\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking ticket counts across states\n",
    "\n",
    "nyc2017_ticket_across_states = nyc2017.select(\"Registration State\").groupBy(\"Registration State\") \\\n",
    "                                    .count().sort(\"count\",ascending=False)\n",
    "\n",
    "nyc2017_ticket_across_states.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above command we observed that the Registration State 99 which is errored contain 16054 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacing the Registration state having '99' value with 'NY' state (state with max entries)\n",
    "\n",
    "nyc2017 = nyc2017.withColumn(\"Registration State\",when(nyc2017[\"Registration State\"]=='99','NY') \\\n",
    "                             .otherwise(nyc2017[\"Registration State\"]))\n",
    "\n",
    "## again extracting distinct Registration State\n",
    "nyc2017.select(countDistinct(col(\"Registration State\"))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial number of distinct states obtained were 65 of which one was errored(with value 0). The error values were replaced with most frequent state 'NY' and then the count of distinct states was reduced to 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's plot ticket counts across registration states to check the distribution now after data correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To plot data first we need to create a data frame with Registration State and count\n",
    "import pandas as pd\n",
    "nyc2017_ticket_across_states = nyc2017.select(\"Registration State\").groupBy(\"Registration State\") \\\n",
    "                                    .count().sort(\"count\",ascending=False)\n",
    "pdDF = nyc2017_ticket_across_states.toPandas()\n",
    "pdDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Display the top 20 stateswith the most no of ticketsalong with their ticket count ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pdDF.plot(x='Registration State',y='count',kind='bar',figsize=(20, 8),legend=None)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Registration State')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Ticket counts across registration states')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NY>NJ>PA>CT>FL>IN>MAVA>MD>NC>TX>IL>GA>AZ>OH>CA>ME>SC>MN>OK  are top 20 states\n",
    "#Values also displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q1 How often does each violation code occur? Display the frequency of the top five violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## register dataframe to temp table\n",
    "\n",
    "nyc2017.createOrReplaceTempView(\"NYC2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting the most frequent violation code along with their annual count\n",
    "\n",
    "nyc2017_violation_code_count = spark.sql(\"SELECT \\\n",
    "                                                 `Violation Code`, \\\n",
    "                                                 count(*) as frequency_per_year \\\n",
    "                                            FROM \\\n",
    "                                                 NYC2017 \\\n",
    "                                            GROUP BY \\\n",
    "                                                 `Violation Code`\\\n",
    "                                             ORDER BY \\\n",
    "                                                 frequency_per_year DESC\")\n",
    "\n",
    "\n",
    "## displaying the top 5 violation codes\n",
    "\n",
    "nyc2017_violation_code_count.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Top five violation codes are 21, 36, 38, 14, 20. Their frequency per year respectively are 768,056 ; 662,765 ; 542,078 ; 476,663 ; 319,644  .\n",
    "\n",
    "Violation code Frequency\n",
    "21 768,056\n",
    "36 662,765\n",
    "38 542,078\n",
    "14 476,663\n",
    "20 319,644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot to see frequency of top 5 violation code across year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To plot data first we need to create a data frame \n",
    "pdDF = nyc2017_violation_code_count.toPandas()\n",
    "\n",
    "## plot to show distribution of ticket counts across different states\n",
    "plt.clf()\n",
    "pdDF[0:5].plot(x='Violation Code',y='frequency_per_year',kind='bar',figsize=(20, 8),legend=None)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Violation Code')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Violation Code frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting the most frequent Vehicle Body Type along with annual count\n",
    "\n",
    "nyc2017_vehicle_body_type_count = spark.sql(\"SELECT \\\n",
    "                                                 `Vehicle Body Type`, \\\n",
    "                                                 count(*) as frequency_per_year \\\n",
    "                                            FROM \\\n",
    "                                                 NYC2017 \\\n",
    "                                            GROUP BY \\\n",
    "                                                 `Vehicle Body Type`\\\n",
    "                                             ORDER BY \\\n",
    "                                                 frequency_per_year DESC\")\n",
    "\n",
    "\n",
    "## displaying the top 5 Vehicle Body Type\n",
    "\n",
    "nyc2017_vehicle_body_type_count.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top five Vehicle Body Type are SUBN, 4DSD, VAN, DELV, SDN."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vehicle body type Frequency\n",
    "SUBN 1,883,925\n",
    "4DSD 1,547,312\n",
    "VAN 724,025\n",
    "DELV 358,980\n",
    "SDN 194,164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To plot data first we need to create a data frame \n",
    "pdDF = nyc2017_vehicle_body_type_count.toPandas()\n",
    "\n",
    "pdDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot to show distribution of ticket counts across top 5 vehicle body\n",
    "plt.clf()\n",
    "pdDF[0:5].plot(x='Vehicle Body Type',y='frequency_per_year',kind='bar',figsize=(20, 8),legend=None)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Vehicle Body Type')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Vehicle Body Type frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting the most frequent Vehicle Make along with annual count\n",
    "\n",
    "nyc2017_vehicle_make_count = spark.sql(\"SELECT \\\n",
    "                                                 `Vehicle Make`, \\\n",
    "                                                 count(*) as frequency_per_year \\\n",
    "                                            FROM \\\n",
    "                                                 NYC2017 \\\n",
    "                                            GROUP BY \\\n",
    "                                                 `Vehicle Make`\\\n",
    "                                             ORDER BY \\\n",
    "                                                 frequency_per_year DESC\")\n",
    "\n",
    "\n",
    "## displaying the top 5 Vehicle Make\n",
    "\n",
    "nyc2017_vehicle_make_count.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top five Vehicle Make are FORD, TOYOT, HONDA, NISSA, CHEVR. Their frequencies per year are mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To plot data first we need to create a data frame \n",
    "pdDF = nyc2017_vehicle_make_count.toPandas()\n",
    "## plot to show distribution of ticket counts across top 5 Vehicle Make\n",
    "plt.clf()\n",
    "pdDF[0:5].plot(x='Vehicle Make',y='frequency_per_year',kind='bar',figsize=(20, 8),legend=None)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Vehicle Make')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Vehicle Make frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Let’s try and find some seasonality in this data:\n",
    "\n",
    "- First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season.\n",
    "\n",
    "- Then, find the three most common violations for each of these seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "nyc2017 = nyc2017.withColumn(\"Issue Month\", month(col(\"Issue Date\")))\n",
    "\n",
    "\n",
    "## creating bucket borders as seasons across 12 months\n",
    "\n",
    "seasonBucketBorders = [1,4,7,10,13]\n",
    "seasonBucket = Bucketizer().setSplits(seasonBucketBorders).setInputCol(\"Issue Month\") \\\n",
    "                    .setOutputCol(\"Season Bucket\")\n",
    "nyc2017 = seasonBucket.transform(nyc2017)\n",
    "\n",
    "nyc2017 = nyc2017.withColumn(\"season\", when(col(\"Season Bucket\")==0,\"WINTER\") \\\n",
    "                                         .otherwise(when(col(\"Season Bucket\")==1,\"SPRING\") \\\n",
    "                                         .otherwise(when(col(\"Season Bucket\")==2,\"SUMMER\") \\\n",
    "                                         .otherwise(\"AUTUMN\"))))\n",
    "\n",
    "nyc2017.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seasons have been assigned based on the month values for Issue date. For months 1-3, season is WINTER, for months 4-6 season is SPRING, for months 7-9 season is SUMMER and for months 10-12 season is AUTUMN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## refresh dataframe to temp table so as to take up new column created in dataframe\n",
    "\n",
    "nyc2017.createOrReplaceTempView(\"NYC2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequencies across seasons\n",
    "\n",
    "nyc2017_season_ticket_count = spark.sql(\"SELECT \\\n",
    "                                                 season, \\\n",
    "                                                 count(1) as frequency_per_year \\\n",
    "                                            FROM \\\n",
    "                                                 NYC2017 \\\n",
    "                                            GROUP BY \\\n",
    "                                                 season\\\n",
    "                                             ORDER BY \\\n",
    "                                                 frequency_per_year DESC\")\n",
    "\n",
    "nyc2017_season_ticket_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequencies for the tickets across each season are mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To plot data first we need to create a data frame \n",
    "pdDF = nyc2017_season_ticket_count.toPandas()\n",
    "## plot to show distribution of ticket counts across different states\n",
    "plt.clf()\n",
    "pdDF.plot(x='season',y='frequency_per_year',kind='bar',figsize=(15, 8),legend=None)\n",
    "plt.xlabel('season')\n",
    "plt.ylabel('frequency')\n",
    "plt.yscale('log')\n",
    "plt.title('season frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Violation hour bucket Frequency\n",
    "Spring 2,760,785\n",
    "Winter 2,669,033\n",
    "Summer 1,046\n",
    "Autumn 970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THEN,find the threemost common violations for each of these seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaulating top 3 violation code across all the seasons\n",
    "\n",
    "## first grouping the data based on season and Violation Code\n",
    "\n",
    "nyc2017_season_violation_code_group = spark.sql(\"SELECT \\\n",
    "                                                     season, \\\n",
    "                                                     `Violation Code`, \\\n",
    "                                                     count(*) as frequency_per_year \\\n",
    "                                            FROM \\\n",
    "                                                     NYC2017 \\\n",
    "                                            GROUP BY \\\n",
    "                                                     season,`Violation Code`\")\n",
    "\n",
    "## creating a sql temp view\n",
    "\n",
    "nyc2017_season_violation_code_group.createOrReplaceTempView(\"NYC2017_SEASON_VIOLATION_CODE_GROUP\")\n",
    "\n",
    "                                                \n",
    "## finally evauluating top3 violation code for each bucket\n",
    "\n",
    "nyc2017_season_violation_code_top3 = spark.sql(\"SELECT \\\n",
    "                                                        season,\\\n",
    "                                                        `Violation Code`, \\\n",
    "                                                        frequency_per_year,\\\n",
    "                                                        rank_top3\\\n",
    "                                                FROM ( \\\n",
    "                                                        SELECT \\\n",
    "                                                                season,\\\n",
    "                                                                `Violation Code`, \\\n",
    "                                                                frequency_per_year, \\\n",
    "                                                                dense_rank() OVER(PARTITION BY season \\\n",
    "                                                                        ORDER BY frequency_per_year DESC) AS rank_top3  \\\n",
    "                                                        FROM \\\n",
    "                                                                NYC2017_SEASON_VIOLATION_CODE_GROUP \\\n",
    "                                                    )  \\\n",
    "                                                WHERE rank_top3 <= 3\")\n",
    "                                    \n",
    "nyc2017_season_violation_code_top3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 validation codes across each season are mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDF = nyc2017_season_violation_code_top3.toPandas()\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x='Violation Code', y='frequency_per_year',hue='season', data=pdDF)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=18);\n",
    "ax.set_title('Top 3 violation code for each season',fontsize=15)\n",
    "ax.set_xlabel(\"violation code\", fontsize=18);\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Season Violation Code Frequency/Year Rank\n",
    "WINTER 21              373,862      1\n",
    "WINTER 36              348,240      2\n",
    "WINTER 38              286,999      3\n",
    "SPRING 21              393,866      1\n",
    "SPRING 36              314,525      2\n",
    "SPRING 38              255,064      3\n",
    "SUMMER 21                  228      1\n",
    "SUMMER 46                  219      2\n",
    "SUMMER 40                  109      3\n",
    "AUTUMN 46                  219      1\n",
    "AUTUMN 40                  121      2\n",
    "AUTUMN 21                  100      3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Let’s take an example of estimating this for the three most commonly occurring codes:\n",
    "\n",
    "- Find the total occurrences of the three most common violation codes.\n",
    "\n",
    "- Then, visit the website:\n",
    "\n",
    "http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "\n",
    "- It lists the fines associated with different violation codes. They’re divided into two categories: one for the highest-density locations in the city and the other for the rest of the city. For the sake of simplicity, take the average of the two.\n",
    "\n",
    "- Using this information, find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection.\n",
    "\n",
    "- What can you intuitively infer from these findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total occurances of top 3 violation codes\n",
    "\n",
    "## top 3 violation codes are 21, 36, 38\n",
    "\n",
    "nyc2017_top3_violation_count = nyc2017.filter(col(\"Violation Code\").isin(21,36,38)).groupBy(col(\"Violation Code\")).count()\n",
    "nyc2017_top3_violation_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the top 3 violation codes avrage fine are as below :\n",
    "\n",
    "- Average fine for Violation code 21 : (65+45)/2  = 55 `$`\n",
    "- Average fine for Violation code 36 : (50+50)/2  = 50 `$`\n",
    "- Average fine for Violation code 38 : (65+35)/2  = 50 `$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now first we need to enter above information into a new column in dataframe\n",
    "\n",
    "## average fine amount for violation code 36 and 38 is 50 and average amount for violation code 21 is 55\n",
    "\n",
    "nyc2017_top3_violation_amount = nyc2017_top3_violation_count.withColumn(\"Average fine per violation\",\\\n",
    "                                                                        when(col(\"Violation Code\")==21,55)\n",
    "                                                                        .otherwise(50))\n",
    "nyc2017_top3_violation_amount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now evaluating the total amount collected\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "## to get the amount collected for each violation code we need to multiply the average amount collected with the \n",
    "## total conunt for each violation code and then we need to to sum up all the product values obtained\n",
    "\n",
    "nyc2017_total_revenue_top3_violation_code = nyc2017_top3_violation_amount.select(sum(col(\"count\")*\\\n",
    "                                                                                      col(\"Average fine per violation\")))\n",
    "nyc2017_total_revenue_top3_violation_code.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The total occurrences for the most three violation codes are shown in below table :\n",
    "Violation Code Frequency/Year\n",
    "38              542,078\n",
    "21              768,056\n",
    "36              662,765\n",
    "As per the information provided in the web link, the average fine for each violation code is\n",
    "evaluated as :\n",
    "· Average fine for Violation code 21 : (65+45)/2 = 55 $\n",
    "· Average fine for Violation code 36 : (50+50)/2 = 50 $\n",
    "· Average fine for Violation code 38 : (65+35)/2 = 50 $\n",
    "Using the above information for frequency and average fine for each violation code, the amount\n",
    "collected is 102,485,230 $.\n",
    "We can infer that fine collection for the violations are good source of revenue generation with\n",
    "big amount of $102,485,230 getting generated from top 3 violations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
